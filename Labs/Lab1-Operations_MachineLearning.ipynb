{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MrBC530gmg5"
   },
   "source": [
    "Let's start with your project: \n",
    "\n",
    "Are you a data scientist? \n",
    "\n",
    "I think you are an awesome a data scientist.\n",
    "\n",
    "### **Problem** \n",
    "**Our goal is to create a predictive model that can answer the following question:**\n",
    "\n",
    "**What kind of people had a better chance of surviving?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhBfwzBgioTD"
   },
   "source": [
    "**Data about passengers:**\n",
    "*   Name\n",
    "*   Age\n",
    "*   Gender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIEH8iZqi-sk"
   },
   "source": [
    "## Install and Import Libraries\n",
    "Let's install PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oj1HhvIOY5Yz"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDp80mG9jmfU"
   },
   "source": [
    "## Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ttzML9fpjE5a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiqECDzLj1Mg"
   },
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn-hxNggkTqV"
   },
   "source": [
    "You have two datasets: \n",
    "* Train  \n",
    "* Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8-A8M7QmKDJ"
   },
   "source": [
    "Read two datasets: \n",
    "* Train\n",
    "* Test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mx2qAccBk15y"
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.csv('train_spark1.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('test_spark1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj2ANTnWmSCq"
   },
   "source": [
    "Let's work with train dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5mWJR30lNs5"
   },
   "source": [
    "**Confirm if this is a dataframe or not:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tEYTePrzk9yl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_df))\n",
    "print(type(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvLJElPrlT4i"
   },
   "source": [
    "**Show 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jYwhqvV8lnO0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Test DF\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-----+--------+\n",
      "|          1|       1|     1|Goldenberg, Mr. S...|  male|49.0|    1|    0|   17453|89.1042|  C92|       C|\n",
      "|          2|       0|     3| Peduzzi, Mr. Joseph|  male|null|    0|    0|A/5 2817|   8.05| null|       S|\n",
      "|          3|       1|     3|  Jalsevac, Mr. Ivan|  male|29.0|    0|    0|  349240| 7.8958| null|       C|\n",
      "|          4|       0|     1|Millet, Mr. Franc...|  male|65.0|    0|    0|   13509|  26.55|  E38|       S|\n",
      "|          5|       1|     1|Kenyon, Mrs. Fred...|female|null|    1|    0|   17464|51.8625|  D21|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train DF')\n",
    "train_df.show(5)\n",
    "print()\n",
    "print('Test DF')\n",
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QIYVxRXlnnw"
   },
   "source": [
    "**Display schema for the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pcvERiICl1Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DF\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "Test DF\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train DF')\n",
    "train_df.printSchema()\n",
    "print('Test DF')\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmE3Wd80l1S6"
   },
   "source": [
    "**Statistical summary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cNY0SItol5Mo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    25%|              223|                  0|                 2|                null|  null|              20.0|                 0|                  0|           19996.0|           7.8958| null|    null|\n",
      "|    50%|              446|                  0|                 3|                null|  null|              28.0|                 0|                  0|          236171.0|          14.4542| null|    null|\n",
      "|    75%|              669|                  1|                 3|                null|  null|              38.0|                 1|                  0|          347743.0|             31.0| null|    null|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiFaIEQTl70_"
   },
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSNPOnP8mw2Q"
   },
   "source": [
    "**Display count for the train dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zrtpG11Fl9HM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_6nnTfxm9_x"
   },
   "source": [
    "**Can you answer this question:** \n",
    "\n",
    "**How many people survived, and how many didn't survive?** \n",
    "\n",
    "**Please save data in a variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QDoqPwyomYxA"
   },
   "outputs": [],
   "source": [
    "trainDF_Survived = train_df.where(col('Survived') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF_UNSurvived = train_df.where(col('Survived') == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8DUtZXPn46m"
   },
   "source": [
    "**Display your result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0XHAK8ceoCMU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF_Survived.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF_UNSurvived.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygsg7wQqor9a"
   },
   "source": [
    "**Can you display your answer in ratio form?(Hint: Use \"UDF\" Function. (Hint: Use \"UDF\" Function. This is a hint you can use any method.)**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3uiaN29PoQnf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived ratio: 38.384 %\n"
     ]
    }
   ],
   "source": [
    "df_cond_count = train_df.where(col('Survived') == 1).count()\n",
    "df_all_count = train_df.count()\n",
    "\n",
    "print('Survived ratio:', round(df_cond_count/df_all_count*100, 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsurvived ratio: 61.616 %\n"
     ]
    }
   ],
   "source": [
    "df_cond_count = train_df.where(col('Survived') == 0).count()\n",
    "df_all_count = train_df.count()\n",
    "\n",
    "print('Unsurvived ratio:', round(df_cond_count/df_all_count*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Aker_lp1h4"
   },
   "source": [
    "**Can you get the number of males and females?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XllkDlo3ongJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Males count: 577\n"
     ]
    }
   ],
   "source": [
    "print('Males count:', train_df.where(col('Sex') == 'male').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Females count: 314\n"
     ]
    }
   ],
   "source": [
    "print('Females count:', train_df.where(col('Sex') == 'female').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHFaJ15zqtEV"
   },
   "source": [
    "**1. What is the average number of survivors of each gender?**\n",
    "\n",
    "**2. What is the number of survivors of each gender?**\n",
    "\n",
    "(Hint: Group by the \"sex\" column. This is a hint you can use any method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NUikH7MUqdKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  233|\n",
      "|  male|  109|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select('Sex')\\\n",
    " .where(col('Survived') == 1)\\\n",
    " .groupBy('Sex')\\\n",
    " .count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|   Sex|Survived|count|\n",
      "+------+--------+-----+\n",
      "|  male|       0|  468|\n",
      "|female|       1|  233|\n",
      "|female|       0|   81|\n",
      "|  male|       1|  109|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy(\"Sex\", \"Survived\").agg(count('*').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------------------+\n",
      "|   Sex|Survived|count|              Ratio|\n",
      "+------+--------+-----+-------------------+\n",
      "|  male|       0|  468| 0.5252525252525253|\n",
      "|female|       1|  233| 0.2615039281705948|\n",
      "|female|       0|   81|0.09090909090909091|\n",
      "|  male|       1|  109|  0.122334455667789|\n",
      "+------+--------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = train_df.groupBy(\"Sex\", \"Survived\").count()\n",
    "\n",
    "df_temp.withColumn('Ratio', expr(\"count\")/train_df.count()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCEdYNdArtRN"
   },
   "source": [
    "**Create temporary view PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YjlK6HDUqsI5"
   },
   "outputs": [],
   "source": [
    "train_df.createOrReplaceTempView(\"tbl_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXNePifnshHr"
   },
   "source": [
    "**How many people survived, and how many didn't survive? By SQL:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0HxfPRTMslqk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Survived, COUNT(*) AS count\n",
    "          FROM tbl_1\n",
    "          GROUP BY Survived\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVCdY6EasFWV"
   },
   "source": [
    "**Can you display the number of survivors from each gender as a ratio?**\n",
    "\n",
    "(Hint: Group by \"sex\" column. This is a hint you can use any method.)\n",
    "\n",
    "**Can you do this via SQL?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7xQc3pUUr3HF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  233|\n",
      "|  male|  109|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Sex, COUNT(*) AS count\n",
    "          FROM tbl_1\n",
    "          WHERE Survived == 1\n",
    "          GROUP BY Sex\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6QXc5V8uu3Y"
   },
   "source": [
    "**Display a ratio for \"p-class\": SUM(Survived)/count for p-class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Mscs2mDFdFsD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|Pclass|              Ratio|\n",
      "+------+-------------------+\n",
      "|     1| 0.6296296296296297|\n",
      "|     3|0.24236252545824846|\n",
      "|     2|0.47282608695652173|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Pclass, SUM(Survived)/COUNT(Pclass) AS Ratio\n",
    "          FROM tbl_1\n",
    "          GROUP BY Pclass\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX0klxwAvg6J"
   },
   "source": [
    "**Let's take a break and continue after this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ctM9t8atxJl"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CfanZTCt6Wk"
   },
   "source": [
    "**First and foremost, we must merge both the train and test datasets. (Hint: The union function can do this.)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8Nm8S1K0r4uY"
   },
   "outputs": [],
   "source": [
    "mergedDF = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI7AD8FLz3iO"
   },
   "source": [
    "**Display count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "s_WERAL8wvJa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R4Miuy0z_uP"
   },
   "source": [
    "**Can you define the number of null values in each column?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0LMOalKBxhpD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|265|    0|    0|     0|   0| 1021|       3|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in mergedDF.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBX8cJ000aqe"
   },
   "source": [
    "**Create Dataframe for null values**\n",
    "\n",
    "1. Column\n",
    "2. Number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ITmyUelNxjJM"
   },
   "outputs": [],
   "source": [
    "nullDF = mergedDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in mergedDF.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(i, nullDF.select(F.collect_list(i)).first()[0][0]) for i in nullDF.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|     Column|Value|\n",
      "+-----------+-----+\n",
      "|PassengerId|    0|\n",
      "|   Survived|    0|\n",
      "|     Pclass|    0|\n",
      "|       Name|    0|\n",
      "|        Sex|    0|\n",
      "|        Age|  265|\n",
      "|      SibSp|    0|\n",
      "|      Parch|    0|\n",
      "|     Ticket|    0|\n",
      "|       Fare|    0|\n",
      "|      Cabin| 1021|\n",
      "|   Embarked|    3|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data,['Column', 'Value']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuKrOi5a0-Ma"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVQlr9vDy7Y4"
   },
   "source": [
    "**Create Temporary view PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xs3yeXhGI8rv"
   },
   "outputs": [],
   "source": [
    "mergedDF.createOrReplaceTempView(\"tbl_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = mergedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txa8NZIO1JaP"
   },
   "source": [
    "**Can you show the \"name\" column from your temporary table?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "m7yXqJoJy35k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Name|\n",
      "+--------------------+\n",
      "|Braund, Mr. Owen ...|\n",
      "|Cumings, Mrs. Joh...|\n",
      "|Heikkinen, Miss. ...|\n",
      "|Futrelle, Mrs. Ja...|\n",
      "|Allen, Mr. Willia...|\n",
      "|    Moran, Mr. James|\n",
      "|McCarthy, Mr. Tim...|\n",
      "|Palsson, Master. ...|\n",
      "|Johnson, Mrs. Osc...|\n",
      "|Nasser, Mrs. Nich...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Name\n",
    "          FROM tbl_2\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|  C85|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F0F9cTZ2Cuz"
   },
   "source": [
    "**Run this code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0kx6OcB-2BBT"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "combined = combined.withColumn('Title',F.regexp_extract(F.col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
    "combined.createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZeUWS12r59"
   },
   "source": [
    "**Display \"Title\" column and count \"Title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "hGkFMtlp1FAI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Title|\n",
      "+------+\n",
      "|    Mr|\n",
      "|   Mrs|\n",
      "|  Miss|\n",
      "|   Mrs|\n",
      "|    Mr|\n",
      "|    Mr|\n",
      "|    Mr|\n",
      "|Master|\n",
      "|   Mrs|\n",
      "|   Mrs|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Title\n",
    "          FROM combined\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|D_Title|\n",
      "+-------+\n",
      "|     17|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT COUNT(DISTINCT(Title)) AS D_Title\n",
    "          FROM combined\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlesDF = spark.sql(\"\"\"SELECT DISTINCT(Title) AS D_Title\n",
    "          FROM combined\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLBQDKYu4JOa"
   },
   "source": [
    "**We can see that Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, and Mme are really rare titles, so create Dictionary and set the value to \"rare\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_list = [\"Dr\", \"Rev\", \"Major\", \"Col\", \"Mlle\", \"Capt\", \"Don\", \"Jonkheer\", \"Countess\", \"Ms\", \"Sir\", \"Lady\", \"Mme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list = titlesDF.select(F.collect_list('D_Title')).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "rjnx5l5r2Qaf"
   },
   "outputs": [],
   "source": [
    "d_titles = {i: 'rare' if i in rare_list else i for i in titles_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wrE95Cv7Oqh"
   },
   "source": [
    "**Run the function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "HdDbWuDl7Pf4"
   },
   "outputs": [],
   "source": [
    "def impute_title(title):\n",
    "    return titles_map[title]# Title_map is your dictionary. please change this name with your dictionary name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5EQVIhK7a9R"
   },
   "source": [
    "**Apply the function on \"Title\" column using UDF:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "rBAiIOn77XFa"
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def impute_title(title):\n",
    "    return d_titles[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.withColumn(\"New_Title\", impute_title(F.col(\"Title\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn8ewllf7kiV"
   },
   "source": [
    "**Display \"Title\" from table and group by \"Title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "J9sjQb084GU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|New_Title|\n",
      "+---------+\n",
      "|       Mr|\n",
      "|      Mrs|\n",
      "|     Miss|\n",
      "|      Mrs|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|   Master|\n",
      "|      Mrs|\n",
      "|      Mrs|\n",
      "|     Miss|\n",
      "|     Miss|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|      Mrs|\n",
      "|   Master|\n",
      "|       Mr|\n",
      "|      Mrs|\n",
      "|      Mrs|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|      Mrs|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|       Mr|\n",
      "|     rare|\n",
      "|      Mrs|\n",
      "|     Miss|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|     Miss|\n",
      "|      Mrs|\n",
      "|      Mrs|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|     Miss|\n",
      "|       Mr|\n",
      "|       Mr|\n",
      "|     Miss|\n",
      "|       Mr|\n",
      "|      Mrs|\n",
      "+---------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.select('New_Title').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|New_Title|count(New_Title)|\n",
      "+---------+----------------+\n",
      "|     rare|              44|\n",
      "|     Miss|             257|\n",
      "|   Master|              56|\n",
      "|       Mr|             786|\n",
      "|      Mrs|             186|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT New_Title, COUNT(New_Title)\n",
    "          FROM combined\n",
    "          GROUP BY New_Title\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H45QNLj9vJp"
   },
   "source": [
    "## **Preprocessing Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwRAhumK-u__"
   },
   "source": [
    "**Based on the \"age\" column mean, you will fill in the missing age values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean as _mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "eXYSVzvl4z63"
   },
   "outputs": [],
   "source": [
    "mean_age = round(combined.select(_mean(col('Age'))).collect()[0][0],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLPivde8_GI-"
   },
   "source": [
    "**Fill missing with \"age\" mean:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "lBgW8aFD90PA"
   },
   "outputs": [],
   "source": [
    "combined = combined.na.fill(value=mean_age,subset=[\"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGsnUz-m_P95"
   },
   "source": [
    "## **Preprocessing Embarked**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHbbamcXMSYP"
   },
   "source": [
    "**Select \"Embarked\" column, count them, order by count Desc, and save in grouped_Embarked variable:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "v-lRu5vc_FW7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|S       |962  |\n",
      "|C       |253  |\n",
      "|Q       |111  |\n",
      "|null    |3    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.select('Embarked', 'Name')\\\n",
    " .groupBy('Embarked')\\\n",
    " .count()\\\n",
    " .orderBy('count',ascending=False)\\\n",
    " .show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_Embarked = combined.select('Embarked', 'Name')\\\n",
    " .groupBy('Embarked')\\\n",
    " .count()\\\n",
    " .orderBy('count',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1qf5u2IOQrx"
   },
   "source": [
    "**Show \"groupped_Embarked\" your variable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "jSFNDTNg_erb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       S|  962|\n",
      "|       C|  253|\n",
      "|       Q|  111|\n",
      "|    null|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_Embarked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzQWYgKBMrbp"
   },
   "source": [
    "**Get max of groupped_Embarked:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = grouped_Embarked.groupby().max('count').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "ZLYj4F7E_iqb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_embarked = grouped_Embarked.filter(expr(\"count\") == max_count).select('Embarked').collect()[0][0]\n",
    "max_embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8vhoEs8N2w_"
   },
   "source": [
    "**Fill missing values with max 'S' of grouped_Embarked:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "LdzQCRud_mAa"
   },
   "outputs": [],
   "source": [
    "combined = combined.na.fill(value=max_embarked,subset=[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEcdV5Vb_qR_"
   },
   "source": [
    "## **Preprocessing Cabin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BQzPs7tqhpA"
   },
   "source": [
    "**Replace \"cabin\" column with first char from the string:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "4b6L5pK0_nQz"
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def replace_char(value):\n",
    "    if value != None:\n",
    "        return value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.withColumn(\"cabin\", replace_char(F.col(\"cabin\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H8XshnYj4k2"
   },
   "source": [
    "**Show the result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "gJUQwnG1Oj2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+-----+-----+-----+----------------+-------+-----+--------+------+---------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|  Age|SibSp|Parch|          Ticket|   Fare|cabin|Embarked| Title|New_Title|\n",
      "+-----------+--------+------+--------------------+------+-----+-----+-----+----------------+-------+-----+--------+------+---------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22.0|    1|    0|       A/5 21171|   7.25| null|       S|    Mr|       Mr|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38.0|    1|    0|        PC 17599|71.2833|    C|       C|   Mrs|      Mrs|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|  Miss|     Miss|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35.0|    1|    0|          113803|   53.1|    C|       S|   Mrs|      Mrs|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35.0|    0|    0|          373450|   8.05| null|       S|    Mr|       Mr|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|30.08|    0|    0|          330877| 8.4583| null|       Q|    Mr|       Mr|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male| 54.0|    0|    0|           17463|51.8625|    E|       S|    Mr|       Mr|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|  2.0|    3|    1|          349909| 21.075| null|       S|Master|   Master|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female| 27.0|    0|    2|          347742|11.1333| null|       S|   Mrs|      Mrs|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female| 14.0|    1|    0|          237736|30.0708| null|       C|   Mrs|      Mrs|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|  4.0|    1|    1|         PP 9549|   16.7|    G|       S|  Miss|     Miss|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female| 58.0|    0|    0|          113783|  26.55|    C|       S|  Miss|     Miss|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male| 20.0|    0|    0|       A/5. 2151|   8.05| null|       S|    Mr|       Mr|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male| 39.0|    1|    5|          347082| 31.275| null|       S|    Mr|       Mr|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female| 14.0|    0|    0|          350406| 7.8542| null|       S|  Miss|     Miss|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female| 55.0|    0|    0|          248706|   16.0| null|       S|   Mrs|      Mrs|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|  2.0|    4|    1|          382652| 29.125| null|       Q|Master|   Master|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|30.08|    0|    0|          244373|   13.0| null|       S|    Mr|       Mr|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female| 31.0|    1|    0|          345763|   18.0| null|       S|   Mrs|      Mrs|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|30.08|    0|    0|            2649|  7.225| null|       C|   Mrs|      Mrs|\n",
      "+-----------+--------+------+--------------------+------+-----+-----+-----+----------------+-------+-----+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzSDsWsUj9Im"
   },
   "source": [
    "**Create the temporary view:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "MR7CXTY7_tMJ"
   },
   "outputs": [],
   "source": [
    "combined.createOrReplaceTempView('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv7lfQFkrLlN"
   },
   "source": [
    "**Select \"Cabin\" column, count \"Cabin\" column, Group by \"Cabin\" column, Order By count DESC**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "A0tZG_mvrKXv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|cabin|count|\n",
      "+-----+-----+\n",
      "| null| 1021|\n",
      "|    C|   82|\n",
      "|    B|   77|\n",
      "|    D|   52|\n",
      "|    E|   51|\n",
      "|    A|   23|\n",
      "|    F|   18|\n",
      "|    G|    4|\n",
      "|    T|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT cabin, COUNT(*) AS count\n",
    "          FROM combined\n",
    "          GROUP BY cabin\n",
    "          ORDER BY count DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GR6j0LOsB4y"
   },
   "source": [
    "**Fill missing values with \"U\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "mwq5CHEz_up_"
   },
   "outputs": [],
   "source": [
    "combined = combined.na.fill(value='U',subset=[\"cabin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRnhA_5-0Hi4"
   },
   "source": [
    "**StringIndexer: A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RIKlOX71GQ-"
   },
   "source": [
    "**StringIndexer(inputCol=None, outputCol=None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "odhuI2EHKuCm"
   },
   "outputs": [],
   "source": [
    "categoricalCols = [field for (field, dataType) in combined.dtypes\n",
    "                   if ((dataType == \"string\") & (field!='Title'))]\n",
    "\n",
    "indexOutputCols = [x + \"_Index\" for x in categoricalCols]\n",
    "\n",
    "oheOutputCols = [x + \"_OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                             outputCols=indexOutputCols,\n",
    "                             handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DECL9yCK3JZ"
   },
   "source": [
    "**OneHotEncoder(inputCols=None, outputCols=None)**\n",
    "\n",
    "A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "tiAjDEy1LBhz"
   },
   "outputs": [],
   "source": [
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                          outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FsiLsd9452v"
   },
   "source": [
    "**VectorAssembler: VectorAssembler(*, inputCols=None, outputCol=None). A feature transformer that merges multiple columns into a vector column.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "BuytKk0hLE6p"
   },
   "outputs": [],
   "source": [
    "numericCols = [field for (field,dataType) in combined.dtypes\n",
    "              if ((dataType!='string') & (field!='Survived'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = oheOutputCols + numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU8DeZfh7JIo"
   },
   "source": [
    "**Use randomSplit function and split data to x_train, and X_test with 80% and 20% Consecutive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "8C11xf1iAzKp"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = combined.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0c_Hf_b0R12"
   },
   "source": [
    "**Pipeline: ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJQvmFai72O7"
   },
   "source": [
    "**Build RandomForestClassifier model and use pipeline to fit and transform then display \"prediction, Survived, features\" columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "YnpmZqlTLPGq"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol='Survived', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, rfc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(X_train)\n",
    "predDF = pipelineModel.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSXEI8-r8bKY"
   },
   "source": [
    "**Use MulticlassClassificationEvaluator and set the \"labelCol\" to \"Survived\",  \"predictionCol\" to \"prediction\", \"metricName\" to \"accuracy\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "Rl0UAKCaBDO-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.783\n"
     ]
    }
   ],
   "source": [
    "Evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',\n",
    "                                         labelCol='Survived',\n",
    "                                         metricName='accuracy')\n",
    "\n",
    "acc = Evaluator.evaluate(predDF)\n",
    "print(f\"Accuracy is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO6_R1zJ9R1R"
   },
   "source": [
    "**When you are finished send the project via Google classroom**\n",
    "**Please let me know if you have any questions.**\n",
    "* nabieh.mostafa@yahoo.com\n",
    "* +201015197566 (Whatsapp)\n",
    "\n",
    "**Don't Hate me, I push you to learn**\n",
    "\n",
    "**I will help you to become an awesome data engineer.**\n",
    "\n",
    "**Why did I say that \"Data Engineer\"?**\n",
    "\n",
    "**Tricky question, but an optional question, if you would like to know the answer, ask me.**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_work.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
